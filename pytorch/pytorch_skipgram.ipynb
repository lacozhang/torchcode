{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import codecs\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "from operator import itemgetter, methodcaller\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptb_train = \"../ptb.train.txt\"\n",
    "ptb_valid = \"../ptb.valid.txt\"\n",
    "ptb_test = \"../ptb.test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, filepath, freq):\n",
    "        self.unk_word = \"<unk>\"\n",
    "        self.unk_id = 0\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.freq = freq\n",
    "        self.text = filepath\n",
    "        self.word2idx[self.unk_word] = self.unk_id\n",
    "        self.idx2word[self.unk_id] = self.unk_word\n",
    "    \n",
    "    def buildDict(self):\n",
    "        logger.info(\"Start extract words\")\n",
    "        counter = defaultdict(int)\n",
    "        for line in codecs.open(self.text, 'r', encoding='utf-8'):\n",
    "            for word in line.strip().split():\n",
    "                counter[word] += 1\n",
    "        logger.info(\"Collect word finished\")\n",
    "        \n",
    "        wordFreqList = [(word, cnt) for word, cnt in counter.items()\n",
    "                       if cnt > self.freq]\n",
    "        sortedWordFreqList = sorted(wordFreqList, key = itemgetter(1), reverse=True)\n",
    "        for word, freq in sortedWordFreqList:\n",
    "            if word not in self.word2idx:\n",
    "                wordIndex = len(self.word2idx)\n",
    "                self.word2idx[word] = wordIndex\n",
    "                self.idx2word[wordIndex] = word\n",
    "        logger.info(\"Vocabulary building finished\")\n",
    "    \n",
    "    def toIdx(self, word):\n",
    "        if word in self.word2idx:\n",
    "            return self.word2idx[word]\n",
    "        else:\n",
    "            logger.warning(\"unknown word {}\".format(word))\n",
    "            return self.unk_id\n",
    "    \n",
    "    def toWord(self, idx):\n",
    "        if isinstance(idx, int):\n",
    "            if idx in self.idx2word:\n",
    "                return self.idx2word[idx]\n",
    "            else:\n",
    "                logger.warning(\"idx {} not found\".format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0719 094734 <ipython-input-4-071325c9a8da>:13] Start extract words\n",
      "I0719 094734 <ipython-input-4-071325c9a8da>:18] Collect word finished\n",
      "I0719 094734 <ipython-input-4-071325c9a8da>:28] Vocabulary building finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "metadata": {
      "bento_obj_id": "140333909434192"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab = Vocabulary(ptb_train, 0)\n",
    "vocab.buildDict()\n",
    "display(len(vocab.word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CBOWDataSet(object):\n",
    "    def __init__(self, filepath, vocab, window_size):\n",
    "        self.window_size = window_size\n",
    "        self.filepath = filepath\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def buildData(self):\n",
    "        self.datasets = []\n",
    "        for line in codecs.open(self.filepath, 'r', 'utf-8'):\n",
    "            words = line.strip().split()\n",
    "            \n",
    "            wordPos2Idx = {}\n",
    "            for wordPos, word in enumerate(words):\n",
    "                wordPos2Idx[wordPos] = self.vocab.toIdx(word)\n",
    "\n",
    "            for wordPos in range(len(words)):\n",
    "                features = []\n",
    "                label = wordPos2Idx[wordPos]\n",
    "                for offset in range(-self.window_size, self.window_size+1):\n",
    "                    if offset == 0:\n",
    "                        continue\n",
    "                    realWordPos = wordPos + offset\n",
    "                    if (realWordPos < 0) or (realWordPos >= len(words)):\n",
    "                        continue\n",
    "                    features.append(wordPos2Idx[wordPos+offset])\n",
    "                if len(features) > 0:\n",
    "                    self.datasets.append((features, label))\n",
    "                if len(self.datasets) % 100000 == 0:\n",
    "                    logger.info(\"Datasets has {} samples\".format(len(self.datasets)))\n",
    "    \n",
    "    def getDataSet(self):\n",
    "        return self.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0719 094748 <ipython-input-6-085db318dc5d>:29] Datasets has 100000 samples\n",
      "I0719 094748 <ipython-input-6-085db318dc5d>:29] Datasets has 200000 samples\n",
      "I0719 094749 <ipython-input-6-085db318dc5d>:29] Datasets has 300000 samples\n",
      "I0719 094749 <ipython-input-6-085db318dc5d>:29] Datasets has 400000 samples\n",
      "I0719 094750 <ipython-input-6-085db318dc5d>:29] Datasets has 500000 samples\n",
      "I0719 094751 <ipython-input-6-085db318dc5d>:29] Datasets has 600000 samples\n",
      "I0719 094751 <ipython-input-6-085db318dc5d>:29] Datasets has 700000 samples\n",
      "I0719 094752 <ipython-input-6-085db318dc5d>:29] Datasets has 800000 samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "887384"
      ]
     },
     "metadata": {
      "bento_obj_id": "140333714537072"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "70377"
      ]
     },
     "metadata": {
      "bento_obj_id": "140333703435888"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "78664"
      ]
     },
     "metadata": {
      "bento_obj_id": "140333690526576"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainData = CBOWDataSet(ptb_train, vocab, 2)\n",
    "trainData.buildData()\n",
    "display(len(trainData.getDataSet()))\n",
    "validData = CBOWDataSet(ptb_valid, vocab, 2)\n",
    "validData.buildData()\n",
    "display(len(validData.getDataSet()))\n",
    "testData = CBOWDataSet(ptb_test, vocab, 2)\n",
    "testData.buildData()\n",
    "display(len(testData.getDataSet()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([9970, 9971], 9969),\n",
       " ([9969, 9971, 9972], 9970),\n",
       " ([9969, 9970, 9972, 9973], 9971),\n",
       " ([9970, 9971, 9973, 9974], 9972),\n",
       " ([9971, 9972, 9974, 9975], 9973),\n",
       " ([9972, 9973, 9975, 9976], 9974),\n",
       " ([9973, 9974, 9976, 9977], 9975),\n",
       " ([9974, 9975, 9977, 9978], 9976),\n",
       " ([9975, 9976, 9978, 9979], 9977),\n",
       " ([9976, 9977, 9979, 9980], 9978)]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "bento_obj_id": "140333875090888"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.getDataSet()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embed = nn.EmbeddingBag(num_embeddings=vocab_size, embedding_dim=embed_size, mode='sum')\n",
    "        self.project = nn.Linear(in_features=embed_size, out_features=vocab_size)\n",
    "        self.offsets = torch.LongTensor([0])\n",
    "    \n",
    "    def forward(self, context):\n",
    "        context_tensor = torch.LongTensor(context)\n",
    "        return F.log_softmax(self.project(self.embed(context_tensor, self.offsets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_ppl(model, dataset):\n",
    "    dataSize = len(dataset)\n",
    "    log_val = 0\n",
    "    with torch.no_grad():\n",
    "        for item in dataset:\n",
    "            predict = model(item[0])\n",
    "            log_val += predict[0, item[1]]\n",
    "        print(\"negative log-likehood {} / ppl {}\".format(log_val, torch.exp(-1.0*log_val/dataSize)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CBOWModel(16, len(vocab.word2idx))\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0720 095829 <ipython-input-63-7d3482511f50>:3] Evaluation without any training\n",
      "I0720 095847 <ipython-input-63-7d3482511f50>:6] Start of 0 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative log-likehood -697692.3125 / ppl 20204.103515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0720 101011 <ipython-input-63-7d3482511f50>:13] Iteration 100000\n",
      "I0720 101011 <ipython-input-63-7d3482511f50>:14] Performance on evaluation data\n",
      "I0720 101028 <ipython-input-63-7d3482511f50>:16] Performance on test data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative log-likehood -560200.0625 / ppl 2864.03759765625\n",
      "negative log-likehood -622617.5 / ppl 2737.765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0720 102233 <ipython-input-63-7d3482511f50>:13] Iteration 200000\n",
      "I0720 102233 <ipython-input-63-7d3482511f50>:14] Performance on evaluation data\n",
      "I0720 102252 <ipython-input-63-7d3482511f50>:16] Performance on test data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative log-likehood -527640.5 / ppl 1803.2449951171875\n",
      "negative log-likehood -586004.0625 / ppl 1718.928955078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0720 103358 <ipython-input-63-7d3482511f50>:13] Iteration 300000\n",
      "I0720 103358 <ipython-input-63-7d3482511f50>:14] Performance on evaluation data\n",
      "I0720 103415 <ipython-input-63-7d3482511f50>:16] Performance on test data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative log-likehood -509649.59375 / ppl 1396.4754638671875\n",
      "negative log-likehood -565990.0 / ppl 1332.7930908203125\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "iter = 0\n",
    "logger.info(\"Evaluation without any training\")\n",
    "evaluate_ppl(model, validData.getDataSet())\n",
    "for i in range(10):\n",
    "    logger.info(\"Start of {} epochs\".format(i))\n",
    "    data = trainData.getDataSet()\n",
    "    indexes = list(range(len(data)))\n",
    "    random.shuffle(indexes)    \n",
    "    for idx in indexes:\n",
    "        iter += 1\n",
    "        if iter%100000 == 0:\n",
    "            logger.info(\"Iteration {}\".format(iter))\n",
    "            logger.info(\"Performance on evaluation data\")\n",
    "            evaluate_ppl(model, validData.getDataSet())\n",
    "            logger.info(\"Performance on test data\")\n",
    "            evaluate_ppl(model, testData.getDataSet())\n",
    "        optimizer.zero_grad()\n",
    "        label = torch.LongTensor([data[idx][1]])\n",
    "        idx_loss = loss(model(data[idx][0]), label)\n",
    "        idx_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    evaluate_ppl(model, validData.getDataSet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative log-likehood -474286.5625 / ppl 415.41192626953125\n"
     ]
    }
   ],
   "source": [
    "evaluate_ppl(model, testData.getDataSet())"
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "bento_kernel_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3rc1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
